# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements. See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License. You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
version: '3'
services:
  grafana:
    image: "grafana/grafana:${GRAFANA_VERSION}"
    ports:
     - "3000:3000"
    environment:
      GF_PATHS_DATA : /var/lib/grafana
      GF_SECURITY_ADMIN_PASSWORD : kafka
    volumes:
     - ./grafana/provisioning:/etc/grafana/provisioning
     - ./grafana/dashboards:/var/lib/grafana/dashboards
    container_name: grafana
    depends_on:
     - prometheus
    networks:
    - kafka

  prometheus:
    image: "prom/prometheus:${PROMETHEUS_VERSION}"
    ports:
     - "9090:9090"
    volumes:
     - ./etc/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
    command: "--config.file=/etc/prometheus/prometheus.yml"
    container_name: prometheus
    networks:
    - kafka

  jmx-kafka101:
    image: "sscaling/jmx-prometheus-exporter:${JMX_EXPORTER_VERSION}"
    environment:
     CONFIG_YML : "/etc/jmx_exporter/config.yml"
     JVM_OPTS: "-Xmx256M -Xms256M"
    volumes:
     - ./etc/jmx_exporter/config_kafka101.yml:/etc/jmx_exporter/config.yml
    container_name: jmx-kafka101
    depends_on:
     - kafka101
    networks:
    - kafka

  jmx-kafka102:
    image: "sscaling/jmx-prometheus-exporter:${JMX_EXPORTER_VERSION}"
    environment:
     CONFIG_YML : "/etc/jmx_exporter/config.yml"
     JVM_OPTS: "-Xmx256M -Xms256M"
    volumes:
     - ./etc/jmx_exporter/config_kafka102.yml:/etc/jmx_exporter/config.yml
    container_name: jmx-kafka102
    depends_on:
     - kafka102
    networks:
    - kafka

  jmx-kafka103:
    image: "sscaling/jmx-prometheus-exporter:${JMX_EXPORTER_VERSION}"
    environment:
     CONFIG_YML : "/etc/jmx_exporter/config.yml"
     JVM_OPTS: "-Xmx256M -Xms256M"
    volumes:
     - ./etc/jmx_exporter/config_kafka103.yml:/etc/jmx_exporter/config.yml
    container_name: jmx-kafka103
    depends_on:
     - kafka103        
    networks:
    - kafka

  jmx-kafka-connect:
    image: "sscaling/jmx-prometheus-exporter:${JMX_EXPORTER_VERSION}"
    environment:
     CONFIG_YML : "/etc/jmx_exporter/config.yml"
     JVM_OPTS: "-Xmx256M -Xms256M"
    volumes:
     - ./etc/jmx_exporter/config_kafka_connect.yml:/etc/jmx_exporter/config.yml
    container_name: jmx-kafka-connect
    depends_on:
     - kafka-connect-1 
    networks:
    - kafka

  create-user:
    image: "confluentinc/cp-kafka:${CONFLUENT_VERSION}"
    restart: on-failure
    depends_on:
      - zk1
      - zk2
      - zk3
    entrypoint: "kafka-configs --zookeeper zk1:2181 --alter --add-config SCRAM-SHA-512='[password=kafka]' --entity-type users --entity-name kafka"
    container_name: create-user
    networks:
      - kafka
  
  create-topic:
    image: confluentinc/cp-kafka:${CONFLUENT_VERSION}
    restart: on-failure
    depends_on:
      - kafka101
      - kafka102
      - kafka103
    volumes:
      - ./etc/clients/client.properties:/client/client.properties
    command: kafka-topics --bootstrap-server kafka101:9092,kafka102:9092,kafka103:9092 --command-config /client/client.properties --create --topic perf --replication-factor 3 --partitions 3
    container_name: create-topic
    networks:
      - kafka

  producer-perf-test:
    image: confluentinc/cp-kafka:${CONFLUENT_VERSION}
    depends_on:
      - kafka101
      - kafka102
      - kafka103
    volumes:
      - ./etc/clients/client.properties:/client/client.properties
    command: kafka-producer-perf-test --producer-props bootstrap.servers=kafka101:9092,kafka102:9092,kafka103:9092 --producer.config /client/client.properties --topic perf --num-records 2000000 --record-size 1000 --throughput 1000000000
    container_name: producer-perf-test
    networks:
      - kafka

  load-connector:
    image: "curlimages/curl:7.69.0"
    hostname: curl
    entrypoint:
      - "/deployConnector.sh"
      - "http://kafka-connect-1:8083"
      - ""
      - "body.json"
    depends_on:
      - kafka-connect-1
    networks:
      - kafka
    restart: on-failure
    volumes:
      - ./bin/connect/deployConnector.sh:/deployConnector.sh:ro
      - ./data/connectorSinkFile.json:/body.json:ro

  zk1:
    image: confluentinc/cp-zookeeper:${CONFLUENT_VERSION}
    environment:
      ZOOKEEPER_SERVER_ID: 1
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_SYNC_LIMIT: 2
      ZOOKEEPER_SERVERS: "zk1:2888:3888;zk2:2888:3888;zk3:2888:3888"
      #ZOOKEEPER_LOG4J_ROOT_LOGLEVEL: DEBUG
    container_name: zk1
    networks:
    - kafka

  zk2:
    image: confluentinc/cp-zookeeper:${CONFLUENT_VERSION}
    environment:
      ZOOKEEPER_SERVER_ID: 2
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_SYNC_LIMIT: 2
      ZOOKEEPER_SERVERS: "zk1:2888:3888;zk2:2888:3888;zk3:2888:3888"
      #ZOOKEEPER_LOG4J_ROOT_LOGLEVEL: DEBUG
    container_name: zk2
    networks:
    - kafka

  zk3:
    image: confluentinc/cp-zookeeper:${CONFLUENT_VERSION}
    environment:
      ZOOKEEPER_SERVER_ID: 3
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_SYNC_LIMIT: 2
      ZOOKEEPER_SERVERS: "zk1:2888:3888;zk2:2888:3888;zk3:2888:3888"
      #ZOOKEEPER_LOG4J_ROOT_LOGLEVEL: DEBUG
    container_name: zk3
    networks:
    - kafka

  kafka101:
    image: confluentinc/cp-kafka:${CONFLUENT_VERSION}
    restart: on-failure
    depends_on:
      - zk1
      - zk2
      - zk3
    environment:
      KAFKA_BROKER_ID: 101
      KAFKA_JMX_PORT: 9991
      KAFKA_ZOOKEEPER_CONNECT: zk1:2181,zk2:2181,zk3:2181
      KAFKA_ADVERTISED_LISTENERS: SECURE://kafka101:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: SECURE:SASL_PLAINTEXT
      KAFKA_LISTENERS: SECURE://:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: SECURE
      KAFKA_AUTHORIZER_CLASS_NAME: kafka.security.auth.SimpleAclAuthorizer
      KAFKA_SUPER_USERS: User:kafka
      KAFKA_SASL_ENABLED_MECHANISMS: SCRAM-SHA-512
      KAFKA_SASL_MECHANISM_INTER_BROKER_PROTOCOL: SCRAM-SHA-512
      KAFKA_LISTENER_NAME_SECURE_SCRAM___sha___512_sasl_jaas_config: "org.apache.kafka.common.security.scram.ScramLoginModule required username=kafka password=kafka;"
      #KAFKA_LOG4J_ROOT_LOGLEVEL: DEBUG
      #KAFKA_TOOLS_LOG4J_LOGLEVEL: DEBUG
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false"
      KAFKA_DELETE_TOPIC_ENABLE: "true"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_HEAP_OPTS: "-Xmx512M -Xms512M"
      CONFLUENT_METRICS_ENABLE: 'false'
    container_name: kafka101
    healthcheck:
      test: nc -z localhost 9092
      interval: 2s
      timeout: 2s
      retries: 3
      start_period: 2s
    networks:
    - kafka

  kafka102:
    image: confluentinc/cp-kafka:${CONFLUENT_VERSION}
    restart: on-failure
    depends_on:
      - zk1
      - zk2
      - zk3
    environment:
      KAFKA_BROKER_ID: 102
      KAFKA_JMX_PORT: 9992
      KAFKA_ZOOKEEPER_CONNECT: zk1:2181,zk2:2181,zk3:2181
      KAFKA_ADVERTISED_LISTENERS: SECURE://kafka102:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: SECURE:SASL_PLAINTEXT
      KAFKA_LISTENERS: SECURE://:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: SECURE
      KAFKA_AUTHORIZER_CLASS_NAME: kafka.security.auth.SimpleAclAuthorizer
      KAFKA_SUPER_USERS: User:kafka
      KAFKA_SASL_ENABLED_MECHANISMS: SCRAM-SHA-512
      KAFKA_SASL_MECHANISM_INTER_BROKER_PROTOCOL: SCRAM-SHA-512
      KAFKA_LISTENER_NAME_SECURE_SCRAM___sha___512_sasl_jaas_config: "org.apache.kafka.common.security.scram.ScramLoginModule required username=kafka password=kafka;"
      #KAFKA_LOG4J_ROOT_LOGLEVEL: DEBUG
      #KAFKA_TOOLS_LOG4J_LOGLEVEL: DEBUG
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false"
      KAFKA_DELETE_TOPIC_ENABLE: "true"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_HEAP_OPTS: "-Xmx512M -Xms512M"
      CONFLUENT_METRICS_ENABLE: 'false'
    container_name: kafka102
    healthcheck:
      test: nc -z localhost 9092
      interval: 2s
      timeout: 2s
      retries: 3
      start_period: 2s
    networks:
    - kafka

  kafka103:
    image: confluentinc/cp-kafka:${CONFLUENT_VERSION}
    restart: on-failure
    depends_on:
      - zk1
      - zk2
      - zk3
    environment:
      KAFKA_BROKER_ID: 103
      KAFKA_JMX_PORT: 9993
      KAFKA_ZOOKEEPER_CONNECT: zk1:2181,zk2:2181,zk3:2181
      KAFKA_ADVERTISED_LISTENERS: SECURE://kafka103:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: SECURE:SASL_PLAINTEXT
      KAFKA_LISTENERS: SECURE://:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: SECURE
      KAFKA_AUTHORIZER_CLASS_NAME: kafka.security.auth.SimpleAclAuthorizer
      KAFKA_SUPER_USERS: User:kafka
      KAFKA_SASL_ENABLED_MECHANISMS: SCRAM-SHA-512
      KAFKA_SASL_MECHANISM_INTER_BROKER_PROTOCOL: SCRAM-SHA-512
      KAFKA_LISTENER_NAME_SECURE_SCRAM___sha___512_sasl_jaas_config: "org.apache.kafka.common.security.scram.ScramLoginModule required username=kafka password=kafka;"
      #KAFKA_LOG4J_ROOT_LOGLEVEL: DEBUG
      #KAFKA_TOOLS_LOG4J_LOGLEVEL: DEBUG
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false"
      KAFKA_DELETE_TOPIC_ENABLE: "true"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_HEAP_OPTS: "-Xmx512M -Xms512M"
      CONFLUENT_METRICS_ENABLE: 'false'
    container_name: kafka103
    healthcheck:
      test: nc -z localhost 9092
      interval: 2s
      timeout: 2s
      retries: 3
      start_period: 2s
    networks:
    - kafka

  kafka-connect-1:
    image: "confluentinc/cp-kafka-connect:${CONFLUENT_VERSION}"
    hostname: kafka-connect-1
    restart: on-failure
    ports:
      - 8083:8083
    depends_on:
      - kafka101
      - kafka102
      - kafka103
    healthcheck:
      test: test `curl -s -o /dev/null -w "%{http_code}" http://localhost:8083/connectors` = 200
      interval: 2s
      timeout: 2s
      retries: 10
      start_period: 2s
    environment:
      - KAFKA_JMX_PORT=9999
      - CONNECT_BOOTSTRAP_SERVERS=kafka101:9092,kafka102:9092,kafka103:9092
      - CONNECT_SECURITY_PROTOCOL=SASL_PLAINTEXT
      - CONNECT_SASL_MECHANISM=SCRAM-SHA-512
      - "CONNECT_SASL_JAAS_CONFIG=org.apache.kafka.common.security.scram.ScramLoginModule required username=kafka password=kafka;"
      - CONNECT_CONSUMER_SASL_MECHANISM=SCRAM-SHA-512
      - CONNECT_PRODUCER_SASL_MECHANISM=SCRAM-SHA-512
      - CONNECT_CONSUMER_SECURITY_PROTOCOL=SASL_PLAINTEXT
      - CONNECT_PRODUCER_SECURITY_PROTOCOL=SASL_PLAINTEXT
      - CONNECT_REST_PORT=8083
      - CONNECT_GROUP_ID=connect
      - CONNECT_CONFIG_STORAGE_TOPIC=_connect-config
      - CONNECT_OFFSET_STORAGE_TOPIC=_connect-offsets
      - CONNECT_STATUS_STORAGE_TOPIC=_connect-status
      - CONNECT_KEY_CONVERTER=org.apache.kafka.connect.json.JsonConverter
      - CONNECT_VALUE_CONVERTER=org.apache.kafka.connect.json.JsonConverter
      - CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE=false
      - CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE=false
      - CONNECT_INTERNAL_KEY_CONVERTER=org.apache.kafka.connect.json.JsonConverter
      - CONNECT_INTERNAL_VALUE_CONVERTER=org.apache.kafka.connect.json.JsonConverter
      - CONNECT_REST_ADVERTISED_HOST_NAME=kafka-connect-1
      - CONNECT_PLUGIN_PATH=/confluent-${CONFLUENT_VERSION}/share/java
      - CONNECT_CONNECTOR_CLIENT_CONFIG_OVERRIDE_POLICY=All
      - CONNECT_HEAP_OPTS="-Xmx512M -Xms512M"
    container_name: kafka-connect-1
    networks:
      - kafka

  schema-registry:
    image: confluentinc/cp-schema-registry:${CONFLUENT_VERSION}
    hostname: schema-registry
    restart: on-failure
    depends_on:
      - kafka101
      - kafka102
      - kafka103
    environment:
      - SCHEMA_REGISTRY_HOST_NAME=schema-registry
      - SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS=kafka101:9092,kafka102:9092,kafka103:9092
      - SCHEMA_REGISTRY_LISTENERS=http://schema-registry:8081
      - SCHEMA_REGISTRY_KAFKASTORE_SECURITY_PROTOCOL=SASL_PLAINTEXT
      - SCHEMA_REGISTRY_KAFKASTORE_SASL_MECHANISM=SCRAM-SHA-512
      - "SCHEMA_REGISTRY_KAFKASTORE_SASL_JAAS_CONFIG=org.apache.kafka.common.security.scram.ScramLoginModule required username=kafka password=kafka;"
      - SCHEMA_REGISTRY_HEAP_OPTS="-Xmx512M -Xms512M"
    container_name: schema-registry
    networks:
      - kafka

  akhq:
    image: tchiotludo/akhq:${AKHQ_VERSION}
    depends_on:
      - kafka101
      - kafka102
      - kafka103
    environment:
     AKHQ_CONFIGURATION: |
        micronaut:
         server:
           cors:
             enabled: true
        akhq:
          connections:
            docker-kafka-server:
              properties:
                bootstrap.servers: "kafka101:9092,kafka102:9092,kafka103:9092"
                security.protocol: SASL_PLAINTEXT
                sasl.mechanism: SCRAM-SHA-512
                sasl.jaas.config: org.apache.kafka.common.security.scram.ScramLoginModule required username="kafka" password="kafka";
              schema-registry:
                url: "http://schema-registry:8081"
              connect:
                - name: connect
                  url: "http://kafka-connect-1:8083"
    ports:
    - 8085:8080
    container_name: akhq
    networks:
    - kafka

networks:
  kafka:
    driver: bridge
